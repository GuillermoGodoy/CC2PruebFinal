{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"escudo_utfsm.gif\" style=\"float:right;height:100px\">\n",
    "<img src=\"IsotipoDIisocolor.png\" style=\"float:left;height:100px\">\n",
    "<center>\n",
    "    <h1> ILI286 Computación Científica II</h1>\n",
    "    <h1> Tarea N°1: Valores propios Numéricos </h1>\n",
    "    <h3> [S]cientific [C]omputing [T]eam 2018</h3>\n",
    "<h4> Hugo Gabriel Parada Ríos  </h4>\n",
    "    <h5> Correo:hugo.parada@sansano.usm.cl           Rol:201510501-4 </h5>\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "<center> 08 de Octubre 2018 </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En 1996, _Larry Page_ y _Sergey Brin_ contactan a los alumnos de Computación Científica II para pedirles ayuda en su nuevo proyecto, que consiste en un algoritmo llamado **PageRank**, el cual pretende ser la base de un motor de búsqueda. 22 años más tarde, Larry y Sergey continúan teniendo problemas dada la gran cantidad de información con la que el algoritmo debe trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo PageRank consiste en modelar la web como un **grafo dirigido**. \n",
    "Consideremos el grafo del texto guía (pag 549):\n",
    "<img src=\"graph.png\"/>\n",
    "\n",
    "En este grafo, cada nodo representa una página y cada arco representa un hipervínculo.\n",
    "Un usuario (_surfer_) hace click en un vínculo, trasladándose a través de los arcos \n",
    "o se teletransporta a cualquier otro nodo de la web.\n",
    "La cantidad de nodos $N$ indica la dimensionalidad de las matrices con las que trabajaremos.\n",
    "\n",
    "Denominaremos **Matriz de Vínculos $A \\in \\mathbb{R}^{N \\times N}$** a la matriz de adyacencia de este grafo, \n",
    "en donde $A_{i,j} = 1$ si y solo si en la página $i$ existe un hipervínculo hacia la página $j$, \n",
    "siendo $A_{i,j} = 0$ en cualquier otro caso. \n",
    "Sin embargo, en esto tipos de grafos pueden existir nodos _sumideros_ los cuales solo tienen hipervínculos hacia el pero no tiene hipervínculos hacia otras páginas.\n",
    "Una consecuencia de esto en nuestro modelo es que los _surfers_ quedan retenidos en esos nodos.\n",
    "Para evitar esta situación, agregamos la siguiente perturbación _rank-one_ a la Matriz de Vínculos $A$,\n",
    "$$\n",
    "    \\mathcal{\\tilde{A}} = \\mathcal{A} + \\mathbf{a}\\cdot\\mathbf{1}^T,\n",
    "$$\n",
    "donde $\\mathbf{a} \\in \\mathbb{R}^{N}$ es un vector con un $1$ en la componente que corresponde a los nodos sumideros  y $0$ en los otras componentes, el vector $\\mathbf{1}$ corresponde al vector de unos en $\\mathbb{R}^{N}$ y $^T$ es el operador transpuesta.\n",
    "\n",
    "Luego definimos la **Matriz de Transiciones $\\mathcal{P} \\in \\mathbb{R}^{N \\times N}$**, \n",
    "la cual contiene la probabilidad de llegar de una página $i$ a una página $j$. Esta se obtiene a partir de la matriz $\\mathcal{\\tilde{A}}$, donde $\\mathcal{P}_{i,j}=\\dfrac{\\mathcal{\\tilde{A}}_{i,j}}{\\sum_{j=1}^N \\mathcal{\\tilde{A}}_{i,j}}$, por lo que satisface:\n",
    "$$\n",
    "    \\sum_{j=1}^N \\mathcal{P}_{i,j} = 1, \\quad \\forall \\, i \\in \\{1,2,\\dots,N\\}.\n",
    "$$\n",
    "Esto significa que si el _surfer_, estando en la página $i$, decide hacer click en un _link_ presente en esta página, lo hará de manera aleatoria y con una distribución de probabilidad uniforme para elegir un hipervínculo, es decir, la probabilidad de utilizar un _link_ de la página $i$ es la misma para cada _link_ existente.\n",
    "\n",
    "Larry y Sergey crearon en 1996 la **Matriz de Google $\\mathcal{G} \\in \\mathbb{R}^{N \\times N}$**, cuya definición es:\n",
    "$$\n",
    "    \\mathcal{G} = (1 - \\alpha) \\cdot \\mathcal{P} + \\alpha \\cdot \\mathcal{\\hat{E}}, \n",
    "$$\n",
    "donde se realiza una combinación convexa entre las matrices $\\mathcal{P}$ y $\\mathcal{\\hat{E}}$, \n",
    "con $\\alpha\\in [0,1]$.\n",
    "Para definir la matriz $\\mathcal{\\hat{E}}\\in\\mathbb{R}^{N \\times N}$, debemos definir primero la matriz $\\mathcal{\\tilde{E}}$:\n",
    "$$\n",
    "    \\mathcal{\\tilde{E}} = \\mathcal{E} + \\mathbf{r}_1\\cdot\\mathbf{r}_2^T,\n",
    "$$\n",
    "donde $\\mathcal{E}\\in\\mathbb{R}^{N \\times N}$ y cada una de sus entradas es $1$, $\\mathbf{r}_1$ y $\\mathbf{r}_2$ son vectores aleatorios cuyos coeficientes siguen una distribión de probabilidad uniforme en el intervalo $\\left[-\\sqrt{\\delta}, \\sqrt{\\delta}\\right]$, con $0 \\leq \\delta < 1$. \n",
    "Así, cada índice de la matriz $\\mathcal{\\hat{E}}$ se obtiene de la siguiente forma:\n",
    "$$\n",
    "    \\mathcal{\\hat{E}}_{i, j} = \\frac{\\mathcal{\\tilde{E}}_{i, j}}{\\sum_{k = 1}^N \\mathcal{\\tilde{E}}_{i, k}}.\n",
    "$$\n",
    "\n",
    "La matriz $\\mathcal{G}$ es una **Matriz estocástica**, lo que significa que la suma de sus filas es $1$.\n",
    "Una consecuencia de esto es que su valor propio dominante es $\\lambda_1 = 1$.\n",
    "\n",
    "Para poder obtener un indicador de la relevancia de cada página, necesitamos obtener el vector de probabilidades estacionarias $\\boldsymbol{\\Pi}$ para la matriz de Google. Si el número de _surfers_ es lo suficientemente grande y luego de que cada usuario haya realizado muchos _cliks_, el porcentaje de la gente presente en cada página \n",
    "será proporcional a la distribución de probabilidad estacionaria de la matriz $\\mathcal{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, buscamos el vector de probabilidades $\\boldsymbol{\\Pi} = \\langle \\pi_1, \\pi_2, \\pi_3, \\dots, \\pi_N \\rangle$. \n",
    "Este se puede computar de manera iterativa mediante el siguiente algoritmo:\n",
    "* Partimos con una distribución no nula cualquiera de los _surfers_ $\\boldsymbol{\\Pi}_0$.\n",
    "* En cada iteración trasladamos a los surfers de acuerdo a las probabilidades definidas, mediante la recurrencia $\\boldsymbol{\\Pi}_k = \\boldsymbol{\\Pi}_{k-1} \\mathcal{G}$ (note que $\\boldsymbol{\\Pi}$ es un vector fila!).\n",
    "* Definimos un criterio de parada basado en el cuociente de Rayleigh y detenemos el algoritmo cuando este se cumpla. La página $i$ que tenga mayor probabilidad $\\pi_i$ es la primera del ranking, y así sucesivamente.\n",
    "\n",
    "Si ponemos atención al algoritmo nos daremos cuenta que... **$\\Pi$ es el vector propio izquierdo dominante de $\\mathcal{G}$!**\n",
    "\n",
    "El objetivo es ocupar los métodos númericos vistos en clases para calcular valores y vectores propios, en pos de resolver el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:50:28.355266Z",
     "start_time": "2018-09-29T05:50:28.191386Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "import scipy.sparse \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Power Iteration (20 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando un pequeño conjunto de datos denominado `small_dataset`, construya el algoritmo de PageRank basándose en Power Iteration. Explique como puede sacar ventaja de conocer que el vector propio dominante de $\\mathcal{G}$ tiene como valor propio asociado $\\lambda_1 = 1$ para definir un buen criterio de parada para el algoritmo y ocupe este criterio.\n",
    "\n",
    "**Importante:** cuando trabajamos con números aleatorios nunca debemos olvidar definir la semilla para que los experimentos sean reproducibles!\n",
    "\n",
    "_Hint: talk to mr. Rayleigh._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo\n",
    "\n",
    "Note que dado que el valor propio dominate es 1, podemos  calcular el cuociente de Rayleigh en cada iteración, y proponer como criterio de parada que el valor absoluto entre el cuociente de Rayleigh y 1 sea menor que cierta tolerancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además procedemos ahora a poner, el algoritmo *Power iteration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(A, x, tol):\n",
    "    error=1000000\n",
    "    while abs(error)>tol:\n",
    "        u = x/np.linalg.norm(x)\n",
    "        x = np.dot (u,A)\n",
    "        lam = np.dot(u, x) \n",
    "        error=1-lam      \n",
    "    u = x/np.linalg.norm(x)\n",
    "    return (lam, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargarmos el conjunto de datos pequeños:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = np.array([\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [2, 0],\n",
    "    [2, 1],\n",
    "    [2, 4],\n",
    "    [3, 4],\n",
    "    [3, 5],\n",
    "    [4, 3],\n",
    "    [4, 5],\n",
    "    [5, 3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En virtud de construir la función \n",
    "notar que la función de distribucion de la uniforme $[a,b]$ es: \n",
    "  $$F(x)=\\left\\{\\begin{matrix}\n",
    "  0 & \\mbox{para }x < a \\\\  \\\\\n",
    "  \\frac{x-a}{b-a} & \\ \\ \\ \\mbox{para }a \\le x < b \\\\  \\\\\n",
    "  1 & \\mbox{para }x \\ge b\n",
    "  \\end{matrix}\\right.\n",
    " \\,\\!$$\n",
    " \n",
    " Luego como $\\textit{np.random}$ me entrega valores entre $[0,1]$ hacemos $y= \\frac{x-a}{b-a} $ luego $x=(b-a)y+a$, en nuestro caso particular tomando $a=-\\sqrt{\\delta}$ y $b=\\sqrt{\\delta}$ se tiene que si $y \\in [0,1]$ luego $x=\\sqrt{\\delta}(2y-1) \\in [-\\sqrt{\\delta},\\sqrt{\\delta}]$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(A, x, tol):\n",
    "    error=1000000\n",
    "    while abs(error)>tol:\n",
    "        u = x/np.linalg.norm(x)\n",
    "        x = np.dot (u,A)\n",
    "        lam = np.dot(u, np.transpose(x)) \n",
    "        error=1-lam      \n",
    "    u = x/np.linalg.norm(x)\n",
    "    return (lam, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration_2(A,E,a,x, tol):\n",
    "    error=1000000\n",
    "    while abs(error)>tol:\n",
    "        u = x/np.linalg.norm(x)\n",
    "        x = (1-a)*np.dot (u,A)+a*np.dot(u,E)\n",
    "        lam = np.dot(u, np.transpose(x)) \n",
    "        error=1-lam        \n",
    "    u = x/np.linalg.norm(x)\n",
    "    return (lam, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentamos ahora el algoritmo inicial para la resolución del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Page_Rank(dataset):\n",
    "    N=dataset.max() #Number of nodes\n",
    "    x0=np.ones(N+1) #vector initial guess for power iteration\n",
    "    A=np.zeros((N+1,N+1)) #Null Matrix\n",
    "    for i,j in dataset: #Construct adjacency matrix\n",
    "        A[i,j]=1\n",
    "    \n",
    "    a=(np.sum(A,  axis=1)==0).astype(np.int) #Vector de sumideros\n",
    "    one=np.ones(N+1) #one vector\n",
    "    \n",
    "\n",
    "    T=A+np.outer(a,one) #one rank perturbation of A\n",
    "    P=T/np.sum(T, axis=1).reshape(N+1,1) #Probability matrix\n",
    "    alpha=0.85\n",
    "    delta=0.15\n",
    "    E=np.ones((N+1,N+1))\n",
    "    r_1,r_2=(delta)**(1/2)*(2*np.random.rand(2,N+1)-1) #random vectors with uniform distribution\n",
    "    E_tilda=E+np.outer(r_1,r_2) #construct Etilda\n",
    "    E_tongo=np.zeros((N+1,N+1))\n",
    "    E_tongo=E_tilda/np.sum(E_tilda, axis=1).reshape(N+1,1) #construct Etongo\n",
    "    G=alpha*P+(1-alpha)*E_tongo #convex combination\n",
    "    l,u=power_iteration(G,x0,10**-6)\n",
    "    \n",
    "    return(u) #return de eigenvector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos nuestro algoritmo con *small_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.22328903, 0.38874713, 0.27768924, 0.61000792, 0.39092841,\n",
       "        0.44381491]), 0.0009703636169433594)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti=time()\n",
    "u_1=Page_Rank(small_dataset)\n",
    "te=time()-ti\n",
    "(u_1,te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que en general el método demora poco en este problema, no sobrepasa los $10^{-3} segundos$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Sparse Matrix (30 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La web resulta en un grafo de conectividad bastante baja, por lo que Larry y Sergey piensan que usted podría beneficiarse de las representaciones de matrices sparse disponibles en el módulo `sparse` de SciPy para reducir el uso de memoria del algoritmo.\n",
    "Utilizando un conjunto de datos más significativo llamado `medium_dataset` (disponible en [este link](https://labcomp.cl/~vlizana/medium_dataset.npy.gz), debe descomprimirlo y cargarlo con `numpy.load`), encuentre y explique una forma de aprovechar esta representación.\n",
    "Luego modifique el algoritmo de la pregunta anterior para que utilize los cambios explicados.\n",
    "\n",
    "Para verificar el desempeño de la modificación, mida el tiempo y espacio en memoria utilizado por su nuevo algoritmo y compárelas utilizando su algoritmo anterior con el _dataset_ de esta pregunta (se recomienda guardar su _Notebook_ antes de ejecutar, además de monitorear en tiempo real el uso de memoria durante este proceso).\n",
    "Analice los resultados obtenidos y concluya.\n",
    "Puede obtener de manera práctica el uso de memoria o aproximarlo de manera teórica.\n",
    "\n",
    "**Advertencia:** De aquí en adelante la ejecución de la tarea se vuelve intensiva en uso de memoria, se recomienda utilizar computadores con harta RAM, como por ejemplo los del laboratorio de computación LabComp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    4,     6],\n",
       "       [   12,    15],\n",
       "       [   12,    16],\n",
       "       ...,\n",
       "       [15826, 15999],\n",
       "       [15850, 15999],\n",
       "       [15962, 15999]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_dataset=np.load('medium_dataset.npy')\n",
    "\n",
    "medium_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las modificaciones para la implementación Sparse del algoritmo, note que la matriz de adyacencia es sparse, luego la perturbación de rango 1 si bien le agrega una cantidad considerable de valores, se puede considerar también como sparse, pues lo importante es el hecho de que los nodos poseen pocos arcos que los unen, luego en power iteration, en ves de trabajar con la combinción convexa separamos las matrices y hacemos los productos por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Page_Rank_Sparse(dataset):\n",
    "    t=time()\n",
    "    flag=True\n",
    "    N=dataset.max() #Number of nodes\n",
    "    x0=np.ones(N+1) #definir un vector sparse\n",
    "    A=np.zeros((N+1,N+1)) #Null Matrix\n",
    "    L=np.zeros((N+1,N+1)) #Null Matrix\n",
    "    for i,j in dataset: #Construct adjacency matrix\n",
    "        A[i,j]=1\n",
    "    sA = sparse.csr_matrix(A)     #We save the matrix like Sparse implementation\n",
    "    \n",
    "    a=(np.sum(sA,  axis=1)==0).astype(np.int) #Vector de sumideros\n",
    "    one=np.ones(N+1) #one vector\n",
    "    T=sA+np.outer(a,one) #one rank perturbation of A\n",
    "    P=T/np.sum(T, axis=1).reshape(N+1,1)\n",
    "    alpha=0.15\n",
    "    delta=0.15\n",
    "    E=np.ones((N+1,N+1))\n",
    "    r_1,r_2=(delta)**(1/2)*(2*np.random.rand(2,N+1)-1) #detalle\n",
    "    E_tilda=E+np.outer(r_1,r_2)\n",
    "    E_tongo=np.zeros((N+1,N+1))\n",
    "    E_tongo=E_tilda/np.sum(E_tilda, axis=1).reshape(N+1,1)\n",
    "    G=(1-alpha)*P+alpha*E_tongo\n",
    "    l,u=power_iteration_2(P,E_tongo,alpha,x0,10**-4)\n",
    "    \n",
    "    \n",
    "    return(u)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el algorimto con nuestra data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0.22334778, 0.38876849, 0.27865438, 0.60991931, 0.38956417,\n",
       "          0.44448258]]), 0.000997304916381836)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti2=time()\n",
    "u_2=Page_Rank_Sparse(small_dataset)\n",
    "te=time()-ti2\n",
    "(u_2,te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que vemos que, realiza el trabajo.Se puede ver que el algoritmo original demora entre 700 seg y 1200 seg en resolver el problema para el medium_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti3=time()\n",
    "u_3=Page_Rank(medium_dataset)\n",
    "te=time()-ti3\n",
    "(u_3,te) #NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti4=time()\n",
    "u_4=Page_Rank_Sparse(medium_dataset)\n",
    "te=time()-ti4\n",
    "(u_4,te) #NOT RUN, my computer dont support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La modificación hecha en la versión sparse empeoró el problema, quizas ya que realiza dos multiplicaciones en ves de una, si bien una de ellas es menos costosa por la estructura sparse, la otra no lo es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación y en pos de seguir trabajando ocuparemos un código ajeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PageRank(dataset, beta=0.85, epsilon=10**-4):\n",
    "    t=time()\n",
    "    N=dataset.max() #Number of nodes\n",
    "    x0=np.ones(N+1) #definir un vector sparse\n",
    "    A=np.zeros((N+1,N+1)) #Null Matrix\n",
    "    L=np.zeros((N+1,N+1)) #Null Matrix\n",
    "    for i,j in dataset: #Construct adjacency matrix\n",
    "        A[i,j]=1\n",
    "    G = sparse.csr_matrix(A)     #We save the matrix like Sparse implementation\n",
    "    #Test adjacency matrix is OK\n",
    "    n,_ = G.shape\n",
    "    assert(G.shape==(n,n))\n",
    "    #Constants Speed-UP\n",
    "    deg_out_beta = G.sum(axis=0).T/beta #vector\n",
    "    #Initialize\n",
    "    ranks = np.ones((n,1))/n #vector\n",
    "    flag = True\n",
    "    while flag:        \n",
    "        with np.errstate(divide='ignore'): # Ignore division by 0 on ranks/deg_out_beta\n",
    "            new_ranks = G.dot((ranks/deg_out_beta)) #vector\n",
    "        #Leaked PageRank\n",
    "        new_ranks += (1-new_ranks.sum())/n\n",
    "        #Stop condition\n",
    "        if np.linalg.norm(ranks-new_ranks,ord=1)<=epsilon:\n",
    "            flag = False        \n",
    "        ranks = new_ranks\n",
    "    return(ranks, time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[3.18607618e-03],\n",
       "         [2.88551129e-03],\n",
       "         [2.89528294e-03],\n",
       "         ...,\n",
       "         [1.43729449e-05],\n",
       "         [1.43729449e-05],\n",
       "         [1.43729449e-05]]), 7.09485650062561)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_PageRank(medium_dataset, beta=0.85, epsilon=10**-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que este algoritmo demora mucho menos tiempo que los anteriores y que ocupa otra estructura en el problema (REF(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- The World Wide Web (40 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.-** Impresionados con los resultados en la _performance_ del algoritmo, Larry y Sergey deciden probarlo con todos los datos que poseen. \n",
    "Utilice el conjunto de datos denominado `large_dataset` (disponible en [este link](https://labcomp.cl/~vlizana/large_dataset.npy.gz), debe descomprimirlo y cargarlo con `numpy.load`) para correr su algoritmo, y luego compare el tiempo de ejecución y el uso de memoria con la ejecución _sparse_ del ítem anterior. \n",
    "Analice los resultados obtenidos y concluya al respecto.\n",
    "\n",
    "**Nota:** Para obtener el puntaje completo en esta pregunta, y en la siguiente, su algoritmo debe demorar menos de **30 segundos** en un computador del Laboratorio de Computación (_a.k.a_ LabComp) para el `large_dataset`.\n",
    "\n",
    "**3.2.-** \n",
    "La competencia de Google está intentando desarrollar un algoritmo que pueda obtener resultados similares a los de PageRank con un costo computacional mucho menor.\n",
    "Larry y Sergey tienen la teoría de que la competencia intentará estimar el _rank_ de cada nodo utilizando el **Grado de Entrada**, es decir, la cantidad de arcos que entran a cada uno de los nodos.\n",
    "Para estudiar si esto es cierto o no, realice un gráfico de dispersión (_scatter plot_) en donde cada punto represente un nodo $i$, siendo la componente $x$ el grado de entrada del nodo y la componente $y$ el PageRank del nodo, es decir, $\\pi_i$.\n",
    "Realice esto para cada combinación posible entre los siguientes valores de $\\alpha$ y $\\delta$.\n",
    "\n",
    "* $\\alpha \\in \\{ 0.15, 0.3, 0.7 \\}$\n",
    "* $\\delta \\in \\{ 0.2, 0.5, 0.8 \\}$\n",
    "\n",
    "A partir de estos gráficos responda:\n",
    "\n",
    "* ¿Deberían preocuparse Larry y Sergey por el algoritmo que está siendo desarrollado por la competencia? ¿Acepta o rechaza Ud. la premisa de la competencia?\n",
    "* ¿Qué puede comentar acerca de la relación entre estas 2 variables? ¿En qué condiciones de $\\alpha$ y $\\delta$ podría funcionar estimar el ranking de una página con el grado?\n",
    "\n",
    "**Bonus único:** El algoritmo más rápido, cuyo tiempo de ejecución sea menor a los 30 segundos, será premiado con 40 puntos extras en la tarea.\n",
    "\n",
    "_This is an actual dataset gathered in 2014 and hosted by the University of Mannheim, you can find the names of the pages with their identifiers in [this file](http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2014-03/webgraph/index.pld.gz)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo \n",
    "Veamos como función el algorimto, compute_page_rank en este caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset=np.load('dataset.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12866975"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_dataset.max() #Cantidad de nodos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-cca2afe0d121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_PageRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlarge_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-65ac8f4a22c5>\u001b[0m in \u001b[0;36mcompute_PageRank\u001b[1;34m(dataset, beta, epsilon)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Number of nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#definir un vector sparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Null Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Null Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#Construct adjacency matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_PageRank(large_dataset, beta=0.85, epsilon=10**-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note que, el sistema no logra guardar la matriz nula de tal numero de nodos, por lo que se debe buscar otra forma de construir la matriz de adyacencia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrucciones:\n",
    "\n",
    "* La estructura del laboratorio es la siguiente:\n",
    "     1. Título, nombre de estudiante, email y rol.\n",
    "     2. Introducción.\n",
    "     3. Desarrollo y análisis de resultados.\n",
    "     4. Conclusiones.\n",
    "     5. Referencias.\n",
    "* El laboratorio debe ser realizado en `Jupyter Notebook` (`Python3`).\n",
    "* Se evaluará la correcta utilización de librerias `NumPy`, `SciPy`, `SymPy`, `Matplotlib` y `ipywidgets`, así como la **correcta implementación de algoritmos vectorizados**.\n",
    "* El archivo de entrega debe denominarse Tarea1-rol.tar.gz y debe contener un directorio con todos los archivos necesarios para ejecutar el notebook, junto con un archivo README indicando explícitamente las librerías o módulos utilizados, nombre y rol del estudiante. El _notebook_ debe tener como nombre Tarea1-rol.ipynb. El no cumplimiento de esta regla significa calificación $0$.\n",
    "* El descuento por día de atraso será de $30$ puntos, con un máximo de 1 día de atraso. No se recibirán entregas después de este día.\n",
    "* Debe citar toda fuente de código externo. \n",
    "* El trabajo es personal, no se permite compartir código ni utilizar código de otros, aunque sí se sugiere discutir aspectos generales con sus compañeros.\n",
    "* En caso de sospecha de no cumplimiento de estas instrucciones, se solicitará al involucrado o la involucrada a aclarar la situación. Dependiendo de la justificación se decidirá su calificación, la cual podrá o no ser penalizada.\n",
    "* El no seguir estas instrucciones, implica descuentos en su nota obtenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    "\n",
    "\n",
    "1.https://es.wikipedia.org/wiki/PageRank\n",
    "\n",
    "2.https://github.com/tclaudioe/Scientific-Computing/tree/master/SC2 (power iteration)\n",
    "\n",
    "3.http://blog.samuelmh.com/2015/02/pagerank-sparse-matrices-python-ipython.html (page rank mejorado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
